session 1 
<<<Single node kubernate Clustor using minikube>> only use in developement or learning purpose not used in production
======
Install miniqube
for windows > check systeminfo >> Hyper -v Requirements should be all yes
download minikube for windows minikube-windows-amd64.exe
install kubectl
create folder in c drive with kubernates and paste downloaded minikube and kubectl
copy path c:/kubernates
advance system setting set environment varible user variable path and paste location
done
rename file in c:/kubernates minkube-windows-amd64.exe to minikube
now run command minikube
it will show the help menu

minikube start >> it will create vm in which kubernate cluster will be setup
done 
kubernates are setup in vm 
kubectl >> kubernates client which help to run commands or operations on kubernate clustor
kubectl get pods >> it will show the pods running
minikube ip >> it will show ip of our running minikube vm
minikube ssh >> it will give ssh access
minikube ssh username = docker
minikube ssh password = tcuser
how to provide dashboard to minikube >> minikube dashboard  command
=======================================================================================
session2
3-node kubernates clustor in vm
we need kubernates master, k8 worker1, k8 worker2. 3 virtual machines
all vm have at least 2 cpu 
all vm are in same network should be able to ping each other
go to vm file preferences>network>addnetwork>network name>ok
vm node setting network selct from list
check ip address of all vms and note down
ssh all 3 vm in ubuntu or kali or wsl this node must be able to ping all 3 vms
ssh username@ip enter then password
every node must have swapoff
to check free -h in all 3 nodes
to disable or off swap > swapoff -a try as root user
this is temporary to make it permananat we need to add in fstab
vim /etc/fstab
commet all lines start with swap in all nodes
we need to install docker in all 3 nodes but note that we should check which version
of docker is supported by k8s to check that follow k8s documentation
install docker in all vms version 19.
sudo cat /sys/class/dmi/id/product_uuid this should be unique in all node
iptables for ubuntu 19.04 or later  
*sudo apt-get update
sudo apt-get install -y apt-transport-https ca-certificates curl

*sudo curl -fsSLo /etc/apt/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg

*echo "deb [signed-by=/etc/apt/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list

*sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl

do this in all nodes



master setup using kubeadm

kubeadm init --pod-network-cidr=10.244.0.0/16

kubectl get nodes to show the nodes in my cluster 
it will not work we need to enter commands to start our cluster
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernates/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(is -g) $HOME/.kube/config

kubectl get nodes >> it will show our nodes in clustor

get pods -A

kubectl apply funnel networking 
get pods -A

kubeadm join wali command copy and paste in worker nodes

kubectl get nodes on master to check all nodes are ready or not



âœ”ğ—£ğ—¼ğ—±: A Pod is the smallest and most basic unit in Kubernetes. It represents a single instance of a running process or a set of tightly coupled processes running together on a node. Pods are scheduled and managed by Kubernetes.

âœ”ğ—–ğ—¹ğ˜‚ğ˜€ğ˜ğ—²ğ—¿: A cluster is a collection of nodes that function as a single entity. It represents every element of the Kubernetes system, including the worker nodes and the control plane parts.

âœ”ğ—¡ğ—¼ğ—±ğ—²: A Node is a Kubernetes cluster's worker computer. Either a real or virtual machine may be used. The operating and management of the pods is done by the nodes.

âœ”ğ—–ğ—¼ğ—»ğ˜ğ—¿ğ—¼ğ—¹-ğ—£ğ—¹ğ—®ğ—»ğ—²: The Kubernetes cluster is managed and controlled by a group of components known as the control plane. It contains the controller manager, scheduler, etcd, and API server.

âœ” ğ—²ğ˜ğ—°ğ—±: Kubernetes uses the distributed key-value store etcd to store configuration and cluster data. For storing cluster state, it offers a dependable and highly accessible data store.

âœ”ğ—¦ğ—°ğ—µğ—²ğ—±ğ˜‚ğ—¹ğ—²ğ—¿: The Scheduler is in charge of allocating pods to nodes in accordance with resource needs, node availability, and other limitations. It selects the appropriate node for scheduling a pod.

âœ”ğ—–ğ—¼ğ—»ğ˜ğ—¿ğ—¼ğ—¹ğ—¹ğ—²ğ—¿-ğ— ğ—®ğ—»ğ—®ğ—´ğ—²ğ—¿: A group of controllers known as the Controller Manager which manages various cluster components. The Node Controller, Replication Controller, and Service Controller are a few examples. They make sure that the cluster's desired and actual states are same.

âœ”ğ—¦ğ—²ğ—¿ğ˜ƒğ—¶ğ—°ğ—²: A logical set of pods and a policy for accessing them are defined by a service, which is an abstraction. For the pods behind it, it offers a reliable network endpoint (IP address) and load balancing.

âœ”ğ—¥ğ—²ğ—½ğ—¹ğ—¶ğ—°ğ—®ğ—¦ğ—²ğ˜: ReplicaSets are Kubernetes objects that guarantee a certain number of pod replicas are active at all times. Scaling and managing the number of pods for a certain application are done using it.

âœ”ğ—¡ğ—®ğ—ºğ—²ğ˜€ğ—½ğ—®ğ—°ğ—²: The cluster resources can be split up into virtual clusters using namespaces. It gives names a range and enables many teams or projects to share the same physical cluster while isolating their resources.
===========================================================================================================================

Chatper=5

ğŸ”¹ Rolling Updates & Rollbacks in Kubernetes=>

Kubernetes supports Rolling Updates to update applications with zero downtime and Rollbacks to revert changes if something goes wrong.
ğŸš€ Rolling Update

A Rolling Update ensures that new versions of an application are deployed gradually, replacing old Pods one at a time to avoid downtime.
âœ… Steps of a Rolling Update:

    Kubernetes starts a new Pod with the updated version.
    It waits for the new Pod to become ready.
    Once ready, it removes an old Pod.
    This repeats until all old Pods are replaced.

ğŸ“Œ Example: Deployment with Rolling Update

apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-app
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1   # Max new pods above the desired count
      maxUnavailable: 1  # Max old pods that can be unavailable during update
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - name: my-app-container
        image: my-app:v1  # Initial version
        ports:
        - containerPort: 80

ğŸš€ How to Perform a Rolling Update?

Update the image of the Deployment:

kubectl set image deployment/my-app my-app-container=my-app:v2

This triggers a Rolling Update to replace my-app:v1 with my-app:v2 gradually.
ğŸ“Œ Check Update Status:

kubectl rollout status deployment/my-app

ğŸ”„ Rollback a Deployment

If an update introduces issues, you can rollback to the previous version.
âœ… Steps of a Rollback:

    Kubernetes stops the faulty Pods.
    It restarts the previous working version.

ğŸš€ How to Perform a Rollback?

Rollback to the last working version:

kubectl rollout undo deployment/my-app

Rollback to a specific revision:

kubectl rollout undo deployment/my-app --to-revision=2

ğŸ“Œ Check Deployment History:

kubectl rollout history deployment/my-app


ChatGPT
ğŸ”„ Jump to a Specific Revision in a Rolling Update

If a Kubernetes Rolling Update has multiple revisions and you need to roll back to a specific version, follow these steps:
âœ… Step 1: Check Deployment History

kubectl rollout history deployment myapp-deployment

This lists all the available revisions, showing metadata for each.

Example Output:

REVISION  CHANGE-CAUSE
1         First deployment
2         Updated image to v2
3         Updated image to v3

âœ… Step 2: Roll Back to a Specific Revision

Use the --to-revision flag:

kubectl rollout undo deployment myapp-deployment --to-revision=2

This will immediately revert the Deployment to revision 2.
âœ… Step 3: Verify the Rollback

Check the status of the Deployment:

kubectl rollout status deployment myapp-deployment

Check the Pods to confirm they are running the correct version:

kubectl get pods -l app=myapp

===================================

Configure Environment Variables in Applications =>


ğŸŒ Configuring Environment Variables in Applications in Kubernetes

Kubernetes allows you to pass environment variables to Pods using different methods:

    Using env in the Deployment YAML
    Using ConfigMaps (for non-sensitive data)
    Using Secrets (for sensitive data like passwords)

âœ… 1. Setting Environment Variables Directly in Deployment YAML

You can specify environment variables inside the Deployment YAML using env:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp-container
        image: myapp:latest
        env:
        - name: APP_ENV
          value: "production"
        - name: DEBUG_MODE
          value: "false"

ğŸ¯ How It Works

    The app container gets APP_ENV=production and DEBUG_MODE=false.

âœ… 2. Using ConfigMap for Environment Variables

A ConfigMap is used to store non-sensitive data like app configuration.
ğŸ“Œ Step 1: Create a ConfigMap

apiVersion: v1
kind: ConfigMap
metadata:
  name: my-config
data:
  APP_ENV: "production"
  DEBUG_MODE: "false"

ğŸ“Œ Step 2: Reference ConfigMap in Deployment

apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp-container
        image: myapp:latest
        envFrom:
        - configMapRef:
            name: my-config

ğŸ¯ How It Works

    The container gets all key-value pairs from my-config as environment variables.

âœ… 3. Using Secrets for Sensitive Data

A Secret is used for storing sensitive information like passwords or API keys.
ğŸ“Œ Step 1: Create a Secret

apiVersion: v1
kind: Secret
metadata:
  name: my-secret
type: Opaque
data:
  DB_PASSWORD: cGFzc3dvcmQ=  # "password" (Base64 encoded)

    âš  Note: Kubernetes requires values to be Base64-encoded.

ğŸ“Œ Step 2: Reference Secret in Deployment

apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp-container
        image: myapp:latest
        env:
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: my-secret
              key: DB_PASSWORD

ğŸ¯ How It Works

    The DB_PASSWORD is fetched from the Secret and injected into the container.

ğŸ”¥ Quick Commands for Applying Configurations

kubectl apply -f configmap.yaml     # Apply ConfigMap
kubectl apply -f secret.yaml        # Apply Secret
kubectl apply -f deployment.yaml    # Apply Deployment

kubectl get configmap my-config -o yaml  # View ConfigMap
kubectl get secret my-secret -o yaml     # View Secret (Base64 encoded)

======================================

Configuring ConfigMaps in Applications=>

ğŸ“Œ Configuring ConfigMaps in Kubernetes Applications

A ConfigMap in Kubernetes is used to store non-sensitive configuration data as key-value pairs. This data can then be injected into Pods as environment variables, command-line arguments, or mounted files.
ğŸš€ 1ï¸âƒ£ Create a ConfigMap

A ConfigMap can be created in three ways:

    From a YAML file (Recommended)
    From command-line arguments
    From an existing file

ğŸ“Œ 1. Creating a ConfigMap using a YAML File

Create a file called configmap.yaml:

apiVersion: v1
kind: ConfigMap
metadata:
  name: my-config
data:
  APP_ENV: "production"
  LOG_LEVEL: "debug"
  MAX_RETRIES: "5"

ğŸ”¹ Apply the ConfigMap

kubectl apply -f configmap.yaml

Check if itâ€™s created:

kubectl get configmap my-config -o yaml

ğŸ“Œ 2. Creating a ConfigMap from CLI

You can create a ConfigMap directly from the command line:

kubectl create configmap my-config --from-literal=APP_ENV=production --from-literal=LOG_LEVEL=debug

List all ConfigMaps:

kubectl get configmaps

ğŸ“Œ 3. Creating a ConfigMap from a File

If you have a configuration file (e.g., app-config.properties):

kubectl create configmap my-config --from-file=app-config.properties

This creates key-value pairs where the file name is the key and the content is the value.
ğŸš€ 2ï¸âƒ£ Using ConfigMaps in Applications

Once a ConfigMap is created, you can use it in Pods in three ways:

    As environment variables
    As command-line arguments
    As mounted files

ğŸ“Œ 1. Use ConfigMap as Environment Variables

Modify your Deployment YAML:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp-container
        image: myapp:latest
        envFrom:
        - configMapRef:
            name: my-config  # Load all key-value pairs as environment variables

ğŸ¯ How It Works

    The container gets all key-value pairs from my-config as environment variables.

To check inside the Pod:

kubectl exec -it <pod-name> -- env | grep APP_ENV

ğŸ“Œ 2. Use ConfigMap as a Single Environment Variable

Instead of loading all keys, you can use a specific key from the ConfigMap:

        env:
        - name: APP_ENV
          valueFrom:
            configMapKeyRef:
              name: my-config
              key: APP_ENV

ğŸ¯ How It Works

    Injects only APP_ENV from my-config.

ğŸ“Œ 3. Use ConfigMap as a Mounted File

You can mount a ConfigMap into a Podâ€™s filesystem.

Modify your Deployment YAML:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp-container
        image: myapp:latest
        volumeMounts:
        - name: config-volume
          mountPath: /etc/config   # Mount the ConfigMap here
      volumes:
      - name: config-volume
        configMap:
          name: my-config

ğŸ¯ How It Works

    Mounts the ConfigMap as files inside the container at /etc/config/.
    Each key in the ConfigMap becomes a file with its value as content.

To check inside the Pod:

kubectl exec -it <pod-name> -- cat /etc/config/APP_ENV
================================================

 Configure Secrets in Applications=>

ğŸ” Configuring Secrets in Kubernetes Applications

Kubernetes Secrets store and manage sensitive information like passwords, API keys, and database credentials securely.
ğŸ”¹ 1ï¸âƒ£ Creating a Secret

You can create Secrets in three ways:

    From a YAML file (Recommended)
    From command-line arguments
    From an existing file

ğŸ“Œ 1. Create a Secret using a YAML File

Create a file called secret.yaml:

apiVersion: v1
kind: Secret
metadata:
  name: my-secret
type: Opaque
data:
  DB_USER: dXNlcm5hbWU=    # Base64 encoded "username"
  DB_PASSWORD: cGFzc3dvcmQ=  # Base64 encoded "password"

    âš  Kubernetes requires values to be Base64-encoded.
    You can encode values using:

echo -n "username" | base64

âœ… Apply the Secret

kubectl apply -f secret.yaml

Verify it:

kubectl get secret my-secret -o yaml

    ğŸ›‘ Values are Base64-encoded, not encrypted.

ğŸ“Œ 2. Create a Secret from CLI

kubectl create secret generic my-secret \
  --from-literal=DB_USER=username \
  --from-literal=DB_PASSWORD=password

List all Secrets:
ğŸ” Configuring Secrets in Kubernetes Applications

Kubernetes Secrets store and manage sensitive information like passwords, API keys, and database credentials securely.
ğŸ”¹ 1ï¸âƒ£ Creating a Secret

You can create Secrets in three ways:

    From a YAML file (Recommended)
    From command-line arguments
    From an existing file

ğŸ“Œ 1. Create a Secret using a YAML File

Create a file called secret.yaml:

apiVersion: v1
kind: Secret
metadata:
  name: my-secret
type: Opaque
data:
  DB_USER: dXNlcm5hbWU=    # Base64 encoded "username"
  DB_PASSWORD: cGFzc3dvcmQ=  # Base64 encoded "password"

    âš  Kubernetes requires values to be Base64-encoded.
    You can encode values using:

echo -n "username" | base64

âœ… Apply the Secret

kubectl apply -f secret.yaml

Verify it:

kubectl get secret my-secret -o yaml

    ğŸ›‘ Values are Base64-encoded, not encrypted.

ğŸ“Œ 2. Create a Secret from CLI

kubectl create secret generic my-secret \
  --from-literal=DB_USER=username \
  --from-literal=DB_PASSWORD=password

List all Secrets:

kubectl get secrets

ğŸ“Œ 3. Create a Secret from a File

If you have a file (e.g., db-creds.txt):

kubectl create secret generic my-secret --from-file=db-creds.txt

ğŸš€ 2ï¸âƒ£ Using Secrets in Applications

Secrets can be injected into Pods in three ways:

    As environment variables
    As mounted files
    As command-line arguments

ğŸ“Œ 1. Use Secret as Environment Variables

Modify your Deployment YAML:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp-container
        image: myapp:latest
        env:
        - name: DB_USER
          valueFrom:
            secretKeyRef:
              name: my-secret
              key: DB_USER
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: my-secret
              key: DB_PASSWORD

ğŸ¯ How It Works

    Injects DB_USER and DB_PASSWORD from my-secret into the container.

To check inside the Pod:

kubectl exec -it <pod-name> -- env | grep DB_

ğŸ“Œ 2. Use Secret as a Mounted File

Modify your Deployment YAML:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp-container
        image: myapp:latest
        volumeMounts:
        - name: secret-volume
          mountPath: "/etc/secrets"
          readOnly: true
      volumes:
      - name: secret-volume
        secret:
          secretName: my-secret

ğŸ¯ How It Works

    Mounts my-secret as files inside the container at /etc/secrets/.
    Each key in the Secret becomes a file, with its value as content.

To check inside the Pod:

kubectl exec -it <pod-name> -- cat /etc/secrets/DB_USER

ğŸ“Œ 3. Use Secret as a Command-Line Argument

Modify your Deployment YAML:

    args: [ "--db-user=$(DB_USER)", "--db-pass=$(DB_PASSWORD)" ]
    env:
    - name: DB_USER
      valueFrom:
        secretKeyRef:
          name: my-secret
          key: DB_USER
    - name: DB_PASSWORD
      valueFrom:
        secretKeyRef:
          name: my-secret
          key: DB_PASSWORD

kubectl get secrets

ğŸ“Œ 3. Create a Secret from a File

If you have a file (e.g., db-creds.txt):

kubectl create secret generic my-secret --from-file=db-creds.txt

ğŸš€ 2ï¸âƒ£ Using Secrets in Applications

Secrets can be injected into Pods in three ways:

    As environment variables
    As mounted files
    As command-line arguments

ğŸ“Œ 1. Use Secret as Environment Variables

Modify your Deployment YAML:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp-container
        image: myapp:latest
        env:
        - name: DB_USER
          valueFrom:
            secretKeyRef:
              name: my-secret
              key: DB_USER
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: my-secret
              key: DB_PASSWORD

ğŸ¯ How It Works

    Injects DB_USER and DB_PASSWORD from my-secret into the container.

To check inside the Pod:

kubectl exec -it <pod-name> -- env | grep DB_

ğŸ“Œ 2. Use Secret as a Mounted File

Modify your Deployment YAML:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp-container
        image: myapp:latest
        volumeMounts:
        - name: secret-volume
          mountPath: "/etc/secrets"
          readOnly: true
      volumes:
      - name: secret-volume
        secret:
          secretName: my-secret

ğŸ¯ How It Works

    Mounts my-secret as files inside the container at /etc/secrets/.
    Each key in the Secret becomes a file, with its value as content.

To check inside the Pod:

kubectl exec -it <pod-name> -- cat /etc/secrets/DB_USER

ğŸ“Œ 3. Use Secret as a Command-Line Argument

Modify your Deployment YAML:

    args: [ "--db-user=$(DB_USER)", "--db-pass=$(DB_PASSWORD)" ]
    env:
    - name: DB_USER
      valueFrom:
        secretKeyRef:
          name: my-secret
          key: DB_USER
    - name: DB_PASSWORD
      valueFrom:
        secretKeyRef:
          name: my-secret
          key: DB_PASSWORD
=================================================================================================================================================
chapter=6

OS Upgrades=>

Upgrading the operating system (OS) in a Kubernetes (k8s) cluster requires careful planning to avoid downtime and ensure compatibility. Hereâ€™s a structured approach:
1. Considerations Before OS Upgrade

    Node Drain: Ensure workloads are safely moved before upgrading.
    Kubernetes Version Compatibility: Check the supported OS versions for your Kubernetes version.
    Upgrade Plan: Decide between rolling upgrades (one node at a time) or a blue-green upgrade (creating new nodes and migrating workloads).
    Backup: Always take backups of critical data (e.g., etcd, application data, configurations).

2. OS Upgrade Process
a. Rolling Upgrade (Recommended)

    Cordon the Node (Prevent scheduling new pods on it):

kubectl cordon <node-name>

Drain the Node (Evict all running pods safely):

kubectl drain <node-name> --ignore-daemonsets --delete-emptydir-data

Upgrade the OS (Apply updates or install a new OS version):

    If using Ubuntu/Debian:

sudo apt update && sudo apt upgrade -y

If using RHEL/CentOS:

    sudo yum update -y

Reboot the Node:

sudo reboot

Uncordon the Node (Allow scheduling of new pods):

kubectl uncordon <node-name>

Verify Node Readiness:

    kubectl get nodes

    Ensure the node returns to the Ready state.

b. Blue-Green Upgrade (For Immutable Infrastructure)

    Create New Nodes with the upgraded OS.
    Join the New Nodes to the cluster.

kubeadm join <control-plane-IP>:6443 --token <token> --discovery-token-ca-cert-hash sha256:<hash>

Migrate Workloads to the new nodes.
Decommission Old Nodes after verifying the new nodes are stable.

    kubectl delete node <old-node-name>

3. Special Cases

    Control Plane Nodes: Upgrade master nodes carefully, one at a time.
    Managed Kubernetes (EKS/AKS/GKE): Use cloud provider tools for upgrading nodes.
    Kernel Live Patching (Advanced): Avoids full reboots if supported by your OS.


=======================

Cluster Upgrade Process=>

Kubernetes Cluster Upgrade Process

Upgrading a Kubernetes (k8s) cluster ensures security, stability, and access to new features. The process depends on whether you use kubeadm, a managed Kubernetes service (EKS/AKS/GKE), or other deployment methods.
1. Pre-Upgrade Checklist

Before upgrading, follow these steps:
âœ… Check Current Kubernetes Version:

kubectl version --short

âœ… Check Available Kubernetes Versions:

kubectl get nodes -o wide
kubeadm upgrade plan

âœ… Backup etcd (if self-managed):

ETCDCTL_API=3 etcdctl snapshot save snapshot.db

âœ… Ensure Cluster Health:

kubectl get nodes
kubectl get pods -A

âœ… Drain Workloads Before Upgrading Nodes:

kubectl cordon <node-name>
kubectl drain <node-name> --ignore-daemonsets --delete-emptydir-data

2. Upgrade Process
A. Upgrade Control Plane (Master Nodes)

1ï¸âƒ£ Update kubeadm:

sudo apt update && sudo apt install -y kubeadm=<new-version>

2ï¸âƒ£ Upgrade Kubernetes Components:

sudo kubeadm upgrade apply <new-version>

3ï¸âƒ£ Update kubelet & kubectl:

sudo apt install -y kubelet=<new-version> kubectl=<new-version>
sudo systemctl restart kubelet

4ï¸âƒ£ Verify the Control Plane Upgrade:

kubectl get nodes
kubectl get pods -A

5ï¸âƒ£ Repeat for All Control Plane Nodes (if applicable)
B. Upgrade Worker Nodes

1ï¸âƒ£ Drain the Node:

kubectl drain <worker-node> --ignore-daemonsets --delete-emptydir-data

2ï¸âƒ£ Upgrade kubeadm on Worker Nodes:

sudo apt update && sudo apt install -y kubeadm=<new-version>
sudo kubeadm upgrade node

3ï¸âƒ£ Upgrade kubelet & kubectl:

sudo apt install -y kubelet=<new-version> kubectl=<new-version>
sudo systemctl restart kubelet

4ï¸âƒ£ Uncordon the Node:

kubectl uncordon <worker-node>

5ï¸âƒ£ Verify Upgrade:

kubectl get nodes
kubectl get pods -A

6ï¸âƒ£ Repeat for All Worker Nodes
3. Post-Upgrade Checks

âœ… Verify Cluster Functionality:

kubectl get nodes
kubectl get pods -A
kubectl get cs

âœ… Upgrade Add-ons & CRDs (if required):

kubectl get pods -n kube-system
kubectl rollout restart deployment -n kube-system

4. Managed Kubernetes Upgrades (EKS/AKS/GKE)

For managed Kubernetes, use the respective cloud provider's upgrade mechanism:

    AWS EKS:

aws eks update-cluster-version --name <cluster-name> --kubernetes-version <new-version>

Azure AKS:

az aks upgrade --resource-group <resource-group> --name <cluster-name> --kubernetes-version <new-version>

Google GKE:

gcloud container clusters upgrade <cluster-name> --cluster-version <new-version>

====================================

how to upgrade kind cluster withour down time=>

Upgrading a kind cluster without downtime requires creating a new cluster and migrating workloads instead of upgrading in place. This approach follows a blue-green deployment model.
Steps to Upgrade a kind Cluster Without Downtime
1. Create a New Cluster with the Desired Kubernetes Version

Create a new cluster with the upgraded Kubernetes version:

kind create cluster --name new-cluster --image kindest/node:v1.27.3

ğŸš€ Replace v1.27.3 with your desired Kubernetes version.
2. Connect Both Clusters Using kubectl Contexts

List available clusters:

kubectl config get-contexts

Set the old and new clusters as variables for easy access:

export OLD_CLUSTER="kind-old-cluster"
export NEW_CLUSTER="kind-new-cluster"

Switch between them as needed:

kubectl config use-context $OLD_CLUSTER
kubectl config use-context $NEW_CLUSTER

3. Migrate Workloads

Use kubectl get all to list running workloads in the old cluster:

kubectl get all -A --context=$OLD_CLUSTER

To migrate all workloads:

kubectl get all -A -o yaml --context=$OLD_CLUSTER | kubectl apply --context=$NEW_CLUSTER -f -

Or, for a more controlled approach, migrate namespaces one by one:

kubectl get ns --context=$OLD_CLUSTER
kubectl get all -n my-namespace -o yaml --context=$OLD_CLUSTER | kubectl apply --context=$NEW_CLUSTER -f -

Verify that workloads are running in the new cluster:

kubectl get pods -A --context=$NEW_CLUSTER

4. Migrate Persistent Data (If Applicable)

If you use PersistentVolumes (PVs), you need to migrate storage. This depends on your storage class:

    For hostPath volumes, manually copy data to the new cluster nodes.
    For external storage (NFS, cloud storage, CSI drivers), ensure the new cluster has access to the same storage.

5. Update Network or Load Balancer

If your applications are exposed using Ingress or a LoadBalancer, update DNS or networking to point to the new cluster.

For example, if using nginx-ingress, migrate it:

kubectl get all -n ingress-nginx -o yaml --context=$OLD_CLUSTER | kubectl apply --context=$NEW_CLUSTER -f -

6. Test and Validate the New Cluster

Before switching traffic, verify: âœ… Pods are running:

kubectl get pods -A --context=$NEW_CLUSTER

âœ… Services are accessible:

kubectl get svc -A --context=$NEW_CLUSTER

âœ… DNS resolution works:

kubectl run --rm -it --image=busybox test -- nslookup my-service.default.svc.cluster.local

7. Switch Traffic to the New Cluster

Once everything is validated:

    Update DNS records (if using an external domain).
    Reconfigure Ingress controllers or load balancers to point to the new cluster.

8. Delete the Old Cluster (After Validation)

Once migration is successful and stable, delete the old cluster:

kind delete cluster --name old-cluster

======================================

Backup and Restore Methods=>

Backup and Restore Methods in Kubernetes (k8s)

Backing up and restoring a Kubernetes cluster ensures disaster recovery, migration, and data protection. Here are the different methods:
1. Backup & Restore Kubernetes Resources (YAML Approach)

This method backs up deployments, services, config maps, secrets, and other k8s objects.
Backup

kubectl get all -A -o yaml > cluster-backup.yaml
kubectl get configmap -A -o yaml > configmaps-backup.yaml
kubectl get secrets -A -o yaml > secrets-backup.yaml

Restore

kubectl apply -f cluster-backup.yaml
kubectl apply -f configmaps-backup.yaml
kubectl apply -f secrets-backup.yaml

ğŸ”¹ Pros: Simple, no extra tools required.
ğŸ”¹ Cons: Doesn't back up persistent data (PVCs, PVs).
2. Backup & Restore Persistent Volumes (PV)

If you use PersistentVolumes (PVs) and PersistentVolumeClaims (PVCs), you need to back up the actual data.
Backup (Using tar & rsync)

    Find the pod using the PVC:

kubectl get pvc

Exec into the pod & copy data:

kubectl exec -it <pod-name> -- tar -czvf /backup.tar.gz /data
kubectl cp <pod-name>:/backup.tar.gz ./backup.tar.gz

Alternatively, use rsync:

    rsync -avz --progress <source-path> <backup-location>

Restore

    Copy data back into the pod:

    kubectl cp backup.tar.gz <pod-name>:/restore.tar.gz
    kubectl exec -it <pod-name> -- tar -xzvf /restore.tar.gz -C /data

ğŸ”¹ Pros: Simple for small clusters.
ğŸ”¹ Cons: Manual process, not ideal for large environments.
3. Etcd Backup & Restore (For Cluster State)

etcd stores Kubernetes cluster state, including API objects, secrets, and configurations.
Backup etcd

ETCDCTL_API=3 etcdctl snapshot save snapshot.db \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key

Restore etcd

    Stop the etcd service:

systemctl stop etcd

Restore the snapshot:

ETCDCTL_API=3 etcdctl snapshot restore snapshot.db

Restart etcd:

    systemctl start etcd

ğŸ”¹ Pros: Full cluster backup.
ğŸ”¹ Cons: Only applicable for self-managed etcd clusters (not for managed Kubernetes like EKS/AKS/GKE).
4. Velero (Recommended for Full Backups)

Velero is an open-source tool for backing up entire Kubernetes clusters, including PVs.
Install Velero

curl -fsSL https://github.com/vmware-tanzu/velero/releases/latest/download/velero-linux-amd64.tar.gz | tar -xzvf -C /usr/local/bin

Backup Cluster

velero backup create my-cluster-backup --include-namespaces=my-namespace

Restore Cluster

velero restore create --from-backup my-cluster-backup

ğŸ”¹ Pros: Automates full cluster backup, works with cloud storage (AWS S3, GCP, Azure).
ğŸ”¹ Cons: Requires setup and external storage.
5. Cloud-Specific Backup Solutions

For managed Kubernetes services, use cloud-native backup tools:

    AWS EKS: AWS Backup, Velero, EBS snapshots
    Azure AKS: Azure Backup, Velero, CSI snapshots
    Google GKE: GKE Backup, Cloud Storage snapshots

============================================













