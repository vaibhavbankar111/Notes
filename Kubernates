session 1 
<<<Single node kubernate Clustor using minikube>> only use in developement or learning purpose not used in production
======
Install miniqube
for windows > check systeminfo >> Hyper -v Requirements should be all yes
download minikube for windows minikube-windows-amd64.exe
install kubectl
create folder in c drive with kubernates and paste downloaded minikube and kubectl
copy path c:/kubernates
advance system setting set environment varible user variable path and paste location
done
rename file in c:/kubernates minkube-windows-amd64.exe to minikube
now run command minikube
it will show the help menu

minikube start >> it will create vm in which kubernate cluster will be setup
done 
kubernates are setup in vm 
kubectl >> kubernates client which help to run commands or operations on kubernate clustor
kubectl get pods >> it will show the pods running
minikube ip >> it will show ip of our running minikube vm
minikube ssh >> it will give ssh access
minikube ssh username = docker
minikube ssh password = tcuser
how to provide dashboard to minikube >> minikube dashboard  command
=======================================================================================
session2
3-node kubernates clustor in vm
we need kubernates master, k8 worker1, k8 worker2. 3 virtual machines
all vm have at least 2 cpu 
all vm are in same network should be able to ping each other
go to vm file preferences>network>addnetwork>network name>ok
vm node setting network selct from list
check ip address of all vms and note down
ssh all 3 vm in ubuntu or kali or wsl this node must be able to ping all 3 vms
ssh username@ip enter then password
every node must have swapoff
to check free -h in all 3 nodes
to disable or off swap > swapoff -a try as root user
this is temporary to make it permananat we need to add in fstab
vim /etc/fstab
commet all lines start with swap in all nodes
we need to install docker in all 3 nodes but note that we should check which version
of docker is supported by k8s to check that follow k8s documentation
install docker in all vms version 19.
sudo cat /sys/class/dmi/id/product_uuid this should be unique in all node
iptables for ubuntu 19.04 or later  
*sudo apt-get update
sudo apt-get install -y apt-transport-https ca-certificates curl

*sudo curl -fsSLo /etc/apt/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg

*echo "deb [signed-by=/etc/apt/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list

*sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl

do this in all nodes



master setup using kubeadm

kubeadm init --pod-network-cidr=10.244.0.0/16

kubectl get nodes to show the nodes in my cluster 
it will not work we need to enter commands to start our cluster
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernates/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(is -g) $HOME/.kube/config

kubectl get nodes >> it will show our nodes in clustor

get pods -A

kubectl apply funnel networking 
get pods -A

kubeadm join wali command copy and paste in worker nodes

kubectl get nodes on master to check all nodes are ready or not



✔𝗣𝗼𝗱: A Pod is the smallest and most basic unit in Kubernetes. It represents a single instance of a running process or a set of tightly coupled processes running together on a node. Pods are scheduled and managed by Kubernetes.

✔𝗖𝗹𝘂𝘀𝘁𝗲𝗿: A cluster is a collection of nodes that function as a single entity. It represents every element of the Kubernetes system, including the worker nodes and the control plane parts.

✔𝗡𝗼𝗱𝗲: A Node is a Kubernetes cluster's worker computer. Either a real or virtual machine may be used. The operating and management of the pods is done by the nodes.

✔𝗖𝗼𝗻𝘁𝗿𝗼𝗹-𝗣𝗹𝗮𝗻𝗲: The Kubernetes cluster is managed and controlled by a group of components known as the control plane. It contains the controller manager, scheduler, etcd, and API server.

✔ 𝗲𝘁𝗰𝗱: Kubernetes uses the distributed key-value store etcd to store configuration and cluster data. For storing cluster state, it offers a dependable and highly accessible data store.

✔𝗦𝗰𝗵𝗲𝗱𝘂𝗹𝗲𝗿: The Scheduler is in charge of allocating pods to nodes in accordance with resource needs, node availability, and other limitations. It selects the appropriate node for scheduling a pod.

✔𝗖𝗼𝗻𝘁𝗿𝗼𝗹𝗹𝗲𝗿-𝗠𝗮𝗻𝗮𝗴𝗲𝗿: A group of controllers known as the Controller Manager which manages various cluster components. The Node Controller, Replication Controller, and Service Controller are a few examples. They make sure that the cluster's desired and actual states are same.

✔𝗦𝗲𝗿𝘃𝗶𝗰𝗲: A logical set of pods and a policy for accessing them are defined by a service, which is an abstraction. For the pods behind it, it offers a reliable network endpoint (IP address) and load balancing.

✔𝗥𝗲𝗽𝗹𝗶𝗰𝗮𝗦𝗲𝘁: ReplicaSets are Kubernetes objects that guarantee a certain number of pod replicas are active at all times. Scaling and managing the number of pods for a certain application are done using it.

✔𝗡𝗮𝗺𝗲𝘀𝗽𝗮𝗰𝗲: The cluster resources can be split up into virtual clusters using namespaces. It gives names a range and enables many teams or projects to share the same physical cluster while isolating their resources.
===========================================================================================================================

Chapter=4 (Logging & Monitoring)

Metrics Server=>

To set up a metrics server in a Kubernetes (K8s) cluster, you can follow these steps. The metrics server collects resource usage data (such as CPU and memory) from the K8s nodes and pods, which is essential for monitoring, autoscaling, and other performance-related operations.

1. Install Metrics Server
You can install the Metrics Server in your Kubernetes cluster by deploying it via a YAML file. The official Metrics Server is available as a Helm chart or can be applied using a manifest from the Kubernetes repository.

Using the Official Kubernetes Metrics Server Deployment:
Run the following command to apply the Metrics Server deployment:

bash
Copy
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.6.1/components.yaml
This will install the Metrics Server into the kube-system namespace.

2. Verify the Installation
Once the installation is complete, you can check the status of the Metrics Server by running:

bash
Copy
kubectl get deployment metrics-server -n kube-system
You should see the metrics-server deployment running.

3. Check Metrics
Once the Metrics Server is running, you can use the following commands to see the metrics:

To get the node metrics (e.g., CPU and memory usage of each node):

bash
Copy
kubectl top nodes
To get the pod metrics (e.g., CPU and memory usage of each pod):

bash
Copy
kubectl top pods
4. Troubleshooting Metrics Server Issues
If you encounter issues where metrics are not appearing or the Metrics Server is not showing the expected data, try the following:

Check Metrics Server logs to identify potential issues:

bash
Copy
kubectl logs -n kube-system deployment/metrics-server
Ensure your API Aggregation Layer is enabled. Metrics Server relies on this component for accessing the metrics API.

Ensure that your K8s nodes allow metrics collection. For example, if you're running on a cloud provider or custom infrastructure, ensure the Kubelet is configured with the correct --read-only-port flag.

5. Use the Metrics Server for Autoscaling
Once the Metrics Server is installed and working, you can use it for Horizontal Pod Autoscaling (HPA). For example, you can scale your application based on CPU usage:

First, create an HPA object:

bash
Copy
kubectl autoscale deployment <deployment-name> --cpu-percent=50 --min=1 --max=10
This will scale your deployment between 1 and 10 replicas based on CPU usage, aiming to maintain an average CPU usage of 50%.


============================================================================================================================
Chatper=5 (Application LifeCycle Management)

🔹 Rolling Updates & Rollbacks in Kubernetes=>

Kubernetes supports Rolling Updates to update applications with zero downtime and Rollbacks to revert changes if something goes wrong.
🚀 Rolling Update

A Rolling Update ensures that new versions of an application are deployed gradually, replacing old Pods one at a time to avoid downtime.
✅ Steps of a Rolling Update:

    Kubernetes starts a new Pod with the updated version.
    It waits for the new Pod to become ready.
    Once ready, it removes an old Pod.
    This repeats until all old Pods are replaced.

📌 Example: Deployment with Rolling Update

apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-app
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1   # Max new pods above the desired count
      maxUnavailable: 1  # Max old pods that can be unavailable during update
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - name: my-app-container
        image: my-app:v1  # Initial version
        ports:
        - containerPort: 80

🚀 How to Perform a Rolling Update?

Update the image of the Deployment:

kubectl set image deployment/my-app my-app-container=my-app:v2

This triggers a Rolling Update to replace my-app:v1 with my-app:v2 gradually.
📌 Check Update Status:

kubectl rollout status deployment/my-app

🔄 Rollback a Deployment

If an update introduces issues, you can rollback to the previous version.
✅ Steps of a Rollback:

    Kubernetes stops the faulty Pods.
    It restarts the previous working version.

🚀 How to Perform a Rollback?

Rollback to the last working version:

kubectl rollout undo deployment/my-app

Rollback to a specific revision:

kubectl rollout undo deployment/my-app --to-revision=2

📌 Check Deployment History:

kubectl rollout history deployment/my-app


ChatGPT
🔄 Jump to a Specific Revision in a Rolling Update

If a Kubernetes Rolling Update has multiple revisions and you need to roll back to a specific version, follow these steps:
✅ Step 1: Check Deployment History

kubectl rollout history deployment myapp-deployment

This lists all the available revisions, showing metadata for each.

Example Output:

REVISION  CHANGE-CAUSE
1         First deployment
2         Updated image to v2
3         Updated image to v3

✅ Step 2: Roll Back to a Specific Revision

Use the --to-revision flag:

kubectl rollout undo deployment myapp-deployment --to-revision=2

This will immediately revert the Deployment to revision 2.
✅ Step 3: Verify the Rollback

Check the status of the Deployment:

kubectl rollout status deployment myapp-deployment

Check the Pods to confirm they are running the correct version:

kubectl get pods -l app=myapp

===================================

Configure Environment Variables in Applications =>


🌍 Configuring Environment Variables in Applications in Kubernetes

Kubernetes allows you to pass environment variables to Pods using different methods:

    Using env in the Deployment YAML
    Using ConfigMaps (for non-sensitive data)
    Using Secrets (for sensitive data like passwords)

✅ 1. Setting Environment Variables Directly in Deployment YAML

You can specify environment variables inside the Deployment YAML using env:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp-container
        image: myapp:latest
        env:
        - name: APP_ENV
          value: "production"
        - name: DEBUG_MODE
          value: "false"

🎯 How It Works

    The app container gets APP_ENV=production and DEBUG_MODE=false.

✅ 2. Using ConfigMap for Environment Variables

A ConfigMap is used to store non-sensitive data like app configuration.
📌 Step 1: Create a ConfigMap

apiVersion: v1
kind: ConfigMap
metadata:
  name: my-config
data:
  APP_ENV: "production"
  DEBUG_MODE: "false"

📌 Step 2: Reference ConfigMap in Deployment

apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp-container
        image: myapp:latest
        envFrom:
        - configMapRef:
            name: my-config

🎯 How It Works

    The container gets all key-value pairs from my-config as environment variables.

✅ 3. Using Secrets for Sensitive Data

A Secret is used for storing sensitive information like passwords or API keys.
📌 Step 1: Create a Secret

apiVersion: v1
kind: Secret
metadata:
  name: my-secret
type: Opaque
data:
  DB_PASSWORD: cGFzc3dvcmQ=  # "password" (Base64 encoded)

    ⚠ Note: Kubernetes requires values to be Base64-encoded.

📌 Step 2: Reference Secret in Deployment

apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp-container
        image: myapp:latest
        env:
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: my-secret
              key: DB_PASSWORD

🎯 How It Works

    The DB_PASSWORD is fetched from the Secret and injected into the container.

🔥 Quick Commands for Applying Configurations

kubectl apply -f configmap.yaml     # Apply ConfigMap
kubectl apply -f secret.yaml        # Apply Secret
kubectl apply -f deployment.yaml    # Apply Deployment

kubectl get configmap my-config -o yaml  # View ConfigMap
kubectl get secret my-secret -o yaml     # View Secret (Base64 encoded)

======================================

Configuring ConfigMaps in Applications=>

📌 Configuring ConfigMaps in Kubernetes Applications

A ConfigMap in Kubernetes is used to store non-sensitive configuration data as key-value pairs. This data can then be injected into Pods as environment variables, command-line arguments, or mounted files.
🚀 1️⃣ Create a ConfigMap

A ConfigMap can be created in three ways:

    From a YAML file (Recommended)
    From command-line arguments
    From an existing file

📌 1. Creating a ConfigMap using a YAML File

Create a file called configmap.yaml:

apiVersion: v1
kind: ConfigMap
metadata:
  name: my-config
data:
  APP_ENV: "production"
  LOG_LEVEL: "debug"
  MAX_RETRIES: "5"

🔹 Apply the ConfigMap

kubectl apply -f configmap.yaml

Check if it’s created:

kubectl get configmap my-config -o yaml

📌 2. Creating a ConfigMap from CLI

You can create a ConfigMap directly from the command line:

kubectl create configmap my-config --from-literal=APP_ENV=production --from-literal=LOG_LEVEL=debug

List all ConfigMaps:

kubectl get configmaps

📌 3. Creating a ConfigMap from a File

If you have a configuration file (e.g., app-config.properties):

kubectl create configmap my-config --from-file=app-config.properties

This creates key-value pairs where the file name is the key and the content is the value.
🚀 2️⃣ Using ConfigMaps in Applications

Once a ConfigMap is created, you can use it in Pods in three ways:

    As environment variables
    As command-line arguments
    As mounted files

📌 1. Use ConfigMap as Environment Variables

Modify your Deployment YAML:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp-container
        image: myapp:latest
        envFrom:
        - configMapRef:
            name: my-config  # Load all key-value pairs as environment variables

🎯 How It Works

    The container gets all key-value pairs from my-config as environment variables.

To check inside the Pod:

kubectl exec -it <pod-name> -- env | grep APP_ENV

📌 2. Use ConfigMap as a Single Environment Variable

Instead of loading all keys, you can use a specific key from the ConfigMap:

        env:
        - name: APP_ENV
          valueFrom:
            configMapKeyRef:
              name: my-config
              key: APP_ENV

🎯 How It Works

    Injects only APP_ENV from my-config.

📌 3. Use ConfigMap as a Mounted File

You can mount a ConfigMap into a Pod’s filesystem.

Modify your Deployment YAML:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp-container
        image: myapp:latest
        volumeMounts:
        - name: config-volume
          mountPath: /etc/config   # Mount the ConfigMap here
      volumes:
      - name: config-volume
        configMap:
          name: my-config

🎯 How It Works

    Mounts the ConfigMap as files inside the container at /etc/config/.
    Each key in the ConfigMap becomes a file with its value as content.

To check inside the Pod:

kubectl exec -it <pod-name> -- cat /etc/config/APP_ENV
================================================

 Configure Secrets in Applications=>

🔐 Configuring Secrets in Kubernetes Applications

Kubernetes Secrets store and manage sensitive information like passwords, API keys, and database credentials securely.
🔹 1️⃣ Creating a Secret

You can create Secrets in three ways:

    From a YAML file (Recommended)
    From command-line arguments
    From an existing file

📌 1. Create a Secret using a YAML File

Create a file called secret.yaml:

apiVersion: v1
kind: Secret
metadata:
  name: my-secret
type: Opaque
data:
  DB_USER: dXNlcm5hbWU=    # Base64 encoded "username"
  DB_PASSWORD: cGFzc3dvcmQ=  # Base64 encoded "password"

    ⚠ Kubernetes requires values to be Base64-encoded.
    You can encode values using:

echo -n "username" | base64

✅ Apply the Secret

kubectl apply -f secret.yaml

Verify it:

kubectl get secret my-secret -o yaml

    🛑 Values are Base64-encoded, not encrypted.

📌 2. Create a Secret from CLI

kubectl create secret generic my-secret \
  --from-literal=DB_USER=username \
  --from-literal=DB_PASSWORD=password

List all Secrets:
🔐 Configuring Secrets in Kubernetes Applications

Kubernetes Secrets store and manage sensitive information like passwords, API keys, and database credentials securely.
🔹 1️⃣ Creating a Secret

You can create Secrets in three ways:

    From a YAML file (Recommended)
    From command-line arguments
    From an existing file

📌 1. Create a Secret using a YAML File

Create a file called secret.yaml:

apiVersion: v1
kind: Secret
metadata:
  name: my-secret
type: Opaque
data:
  DB_USER: dXNlcm5hbWU=    # Base64 encoded "username"
  DB_PASSWORD: cGFzc3dvcmQ=  # Base64 encoded "password"

    ⚠ Kubernetes requires values to be Base64-encoded.
    You can encode values using:

echo -n "username" | base64

✅ Apply the Secret

kubectl apply -f secret.yaml

Verify it:

kubectl get secret my-secret -o yaml

    🛑 Values are Base64-encoded, not encrypted.

📌 2. Create a Secret from CLI

kubectl create secret generic my-secret \
  --from-literal=DB_USER=username \
  --from-literal=DB_PASSWORD=password

List all Secrets:

kubectl get secrets

📌 3. Create a Secret from a File

If you have a file (e.g., db-creds.txt):

kubectl create secret generic my-secret --from-file=db-creds.txt

🚀 2️⃣ Using Secrets in Applications

Secrets can be injected into Pods in three ways:

    As environment variables
    As mounted files
    As command-line arguments

📌 1. Use Secret as Environment Variables

Modify your Deployment YAML:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp-container
        image: myapp:latest
        env:
        - name: DB_USER
          valueFrom:
            secretKeyRef:
              name: my-secret
              key: DB_USER
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: my-secret
              key: DB_PASSWORD

🎯 How It Works

    Injects DB_USER and DB_PASSWORD from my-secret into the container.

To check inside the Pod:

kubectl exec -it <pod-name> -- env | grep DB_

📌 2. Use Secret as a Mounted File

Modify your Deployment YAML:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp-container
        image: myapp:latest
        volumeMounts:
        - name: secret-volume
          mountPath: "/etc/secrets"
          readOnly: true
      volumes:
      - name: secret-volume
        secret:
          secretName: my-secret

🎯 How It Works

    Mounts my-secret as files inside the container at /etc/secrets/.
    Each key in the Secret becomes a file, with its value as content.

To check inside the Pod:

kubectl exec -it <pod-name> -- cat /etc/secrets/DB_USER

📌 3. Use Secret as a Command-Line Argument

Modify your Deployment YAML:

    args: [ "--db-user=$(DB_USER)", "--db-pass=$(DB_PASSWORD)" ]
    env:
    - name: DB_USER
      valueFrom:
        secretKeyRef:
          name: my-secret
          key: DB_USER
    - name: DB_PASSWORD
      valueFrom:
        secretKeyRef:
          name: my-secret
          key: DB_PASSWORD

kubectl get secrets

📌 3. Create a Secret from a File

If you have a file (e.g., db-creds.txt):

kubectl create secret generic my-secret --from-file=db-creds.txt

🚀 2️⃣ Using Secrets in Applications

Secrets can be injected into Pods in three ways:

    As environment variables
    As mounted files
    As command-line arguments

📌 1. Use Secret as Environment Variables

Modify your Deployment YAML:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp-container
        image: myapp:latest
        env:
        - name: DB_USER
          valueFrom:
            secretKeyRef:
              name: my-secret
              key: DB_USER
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: my-secret
              key: DB_PASSWORD

🎯 How It Works

    Injects DB_USER and DB_PASSWORD from my-secret into the container.

To check inside the Pod:

kubectl exec -it <pod-name> -- env | grep DB_

📌 2. Use Secret as a Mounted File

Modify your Deployment YAML:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp-container
        image: myapp:latest
        volumeMounts:
        - name: secret-volume
          mountPath: "/etc/secrets"
          readOnly: true
      volumes:
      - name: secret-volume
        secret:
          secretName: my-secret

🎯 How It Works

    Mounts my-secret as files inside the container at /etc/secrets/.
    Each key in the Secret becomes a file, with its value as content.

To check inside the Pod:

kubectl exec -it <pod-name> -- cat /etc/secrets/DB_USER

📌 3. Use Secret as a Command-Line Argument

Modify your Deployment YAML:

    args: [ "--db-user=$(DB_USER)", "--db-pass=$(DB_PASSWORD)" ]
    env:
    - name: DB_USER
      valueFrom:
        secretKeyRef:
          name: my-secret
          key: DB_USER
    - name: DB_PASSWORD
      valueFrom:
        secretKeyRef:
          name: my-secret
          key: DB_PASSWORD
=================================================================================================================================================
chapter=6 (Cluster Maintenance)

OS Upgrades=>

Upgrading the operating system (OS) in a Kubernetes (k8s) cluster requires careful planning to avoid downtime and ensure compatibility. Here’s a structured approach:
1. Considerations Before OS Upgrade

    Node Drain: Ensure workloads are safely moved before upgrading.
    Kubernetes Version Compatibility: Check the supported OS versions for your Kubernetes version.
    Upgrade Plan: Decide between rolling upgrades (one node at a time) or a blue-green upgrade (creating new nodes and migrating workloads).
    Backup: Always take backups of critical data (e.g., etcd, application data, configurations).

2. OS Upgrade Process
a. Rolling Upgrade (Recommended)

    Cordon the Node (Prevent scheduling new pods on it):

kubectl cordon <node-name>

Drain the Node (Evict all running pods safely):

kubectl drain <node-name> --ignore-daemonsets --delete-emptydir-data

Upgrade the OS (Apply updates or install a new OS version):

    If using Ubuntu/Debian:

sudo apt update && sudo apt upgrade -y

If using RHEL/CentOS:

    sudo yum update -y

Reboot the Node:

sudo reboot

Uncordon the Node (Allow scheduling of new pods):

kubectl uncordon <node-name>

Verify Node Readiness:

    kubectl get nodes

    Ensure the node returns to the Ready state.

b. Blue-Green Upgrade (For Immutable Infrastructure)

    Create New Nodes with the upgraded OS.
    Join the New Nodes to the cluster.

kubeadm join <control-plane-IP>:6443 --token <token> --discovery-token-ca-cert-hash sha256:<hash>

Migrate Workloads to the new nodes.
Decommission Old Nodes after verifying the new nodes are stable.

    kubectl delete node <old-node-name>

3. Special Cases

    Control Plane Nodes: Upgrade master nodes carefully, one at a time.
    Managed Kubernetes (EKS/AKS/GKE): Use cloud provider tools for upgrading nodes.
    Kernel Live Patching (Advanced): Avoids full reboots if supported by your OS.


=======================

Cluster Upgrade Process=>

Kubernetes Cluster Upgrade Process

Upgrading a Kubernetes (k8s) cluster ensures security, stability, and access to new features. The process depends on whether you use kubeadm, a managed Kubernetes service (EKS/AKS/GKE), or other deployment methods.
1. Pre-Upgrade Checklist

Before upgrading, follow these steps:
✅ Check Current Kubernetes Version:

kubectl version --short

✅ Check Available Kubernetes Versions:

kubectl get nodes -o wide
kubeadm upgrade plan

✅ Backup etcd (if self-managed):

ETCDCTL_API=3 etcdctl snapshot save snapshot.db

✅ Ensure Cluster Health:

kubectl get nodes
kubectl get pods -A

✅ Drain Workloads Before Upgrading Nodes:

kubectl cordon <node-name>
kubectl drain <node-name> --ignore-daemonsets --delete-emptydir-data

2. Upgrade Process
A. Upgrade Control Plane (Master Nodes)

1️⃣ Update kubeadm:

sudo apt update && sudo apt install -y kubeadm=<new-version>

2️⃣ Upgrade Kubernetes Components:

sudo kubeadm upgrade apply <new-version>

3️⃣ Update kubelet & kubectl:

sudo apt install -y kubelet=<new-version> kubectl=<new-version>
sudo systemctl restart kubelet

4️⃣ Verify the Control Plane Upgrade:

kubectl get nodes
kubectl get pods -A

5️⃣ Repeat for All Control Plane Nodes (if applicable)
B. Upgrade Worker Nodes

1️⃣ Drain the Node:

kubectl drain <worker-node> --ignore-daemonsets --delete-emptydir-data

2️⃣ Upgrade kubeadm on Worker Nodes:

sudo apt update && sudo apt install -y kubeadm=<new-version>
sudo kubeadm upgrade node

3️⃣ Upgrade kubelet & kubectl:

sudo apt install -y kubelet=<new-version> kubectl=<new-version>
sudo systemctl restart kubelet

4️⃣ Uncordon the Node:

kubectl uncordon <worker-node>

5️⃣ Verify Upgrade:

kubectl get nodes
kubectl get pods -A

6️⃣ Repeat for All Worker Nodes
3. Post-Upgrade Checks

✅ Verify Cluster Functionality:

kubectl get nodes
kubectl get pods -A
kubectl get cs

✅ Upgrade Add-ons & CRDs (if required):

kubectl get pods -n kube-system
kubectl rollout restart deployment -n kube-system

4. Managed Kubernetes Upgrades (EKS/AKS/GKE)

For managed Kubernetes, use the respective cloud provider's upgrade mechanism:

    AWS EKS:

aws eks update-cluster-version --name <cluster-name> --kubernetes-version <new-version>

Azure AKS:

az aks upgrade --resource-group <resource-group> --name <cluster-name> --kubernetes-version <new-version>

Google GKE:

gcloud container clusters upgrade <cluster-name> --cluster-version <new-version>

====================================

how to upgrade kind cluster withour down time=>

Upgrading a kind cluster without downtime requires creating a new cluster and migrating workloads instead of upgrading in place. This approach follows a blue-green deployment model.
Steps to Upgrade a kind Cluster Without Downtime
1. Create a New Cluster with the Desired Kubernetes Version

Create a new cluster with the upgraded Kubernetes version:

kind create cluster --name new-cluster --image kindest/node:v1.27.3

🚀 Replace v1.27.3 with your desired Kubernetes version.
2. Connect Both Clusters Using kubectl Contexts

List available clusters:

kubectl config get-contexts

Set the old and new clusters as variables for easy access:

export OLD_CLUSTER="kind-old-cluster"
export NEW_CLUSTER="kind-new-cluster"

Switch between them as needed:

kubectl config use-context $OLD_CLUSTER
kubectl config use-context $NEW_CLUSTER

3. Migrate Workloads

Use kubectl get all to list running workloads in the old cluster:

kubectl get all -A --context=$OLD_CLUSTER

To migrate all workloads:

kubectl get all -A -o yaml --context=$OLD_CLUSTER | kubectl apply --context=$NEW_CLUSTER -f -

Or, for a more controlled approach, migrate namespaces one by one:

kubectl get ns --context=$OLD_CLUSTER
kubectl get all -n my-namespace -o yaml --context=$OLD_CLUSTER | kubectl apply --context=$NEW_CLUSTER -f -

Verify that workloads are running in the new cluster:

kubectl get pods -A --context=$NEW_CLUSTER

4. Migrate Persistent Data (If Applicable)

If you use PersistentVolumes (PVs), you need to migrate storage. This depends on your storage class:

    For hostPath volumes, manually copy data to the new cluster nodes.
    For external storage (NFS, cloud storage, CSI drivers), ensure the new cluster has access to the same storage.

5. Update Network or Load Balancer

If your applications are exposed using Ingress or a LoadBalancer, update DNS or networking to point to the new cluster.

For example, if using nginx-ingress, migrate it:

kubectl get all -n ingress-nginx -o yaml --context=$OLD_CLUSTER | kubectl apply --context=$NEW_CLUSTER -f -

6. Test and Validate the New Cluster

Before switching traffic, verify: ✅ Pods are running:

kubectl get pods -A --context=$NEW_CLUSTER

✅ Services are accessible:

kubectl get svc -A --context=$NEW_CLUSTER

✅ DNS resolution works:

kubectl run --rm -it --image=busybox test -- nslookup my-service.default.svc.cluster.local

7. Switch Traffic to the New Cluster

Once everything is validated:

    Update DNS records (if using an external domain).
    Reconfigure Ingress controllers or load balancers to point to the new cluster.

8. Delete the Old Cluster (After Validation)

Once migration is successful and stable, delete the old cluster:

kind delete cluster --name old-cluster

======================================

Backup and Restore Methods=>

Backup and Restore Methods in Kubernetes (k8s)

Backing up and restoring a Kubernetes cluster ensures disaster recovery, migration, and data protection. Here are the different methods:
1. Backup & Restore Kubernetes Resources (YAML Approach)

This method backs up deployments, services, config maps, secrets, and other k8s objects.
Backup

kubectl get all -A -o yaml > cluster-backup.yaml
kubectl get configmap -A -o yaml > configmaps-backup.yaml
kubectl get secrets -A -o yaml > secrets-backup.yaml

Restore

kubectl apply -f cluster-backup.yaml
kubectl apply -f configmaps-backup.yaml
kubectl apply -f secrets-backup.yaml

🔹 Pros: Simple, no extra tools required.
🔹 Cons: Doesn't back up persistent data (PVCs, PVs).
2. Backup & Restore Persistent Volumes (PV)

If you use PersistentVolumes (PVs) and PersistentVolumeClaims (PVCs), you need to back up the actual data.
Backup (Using tar & rsync)

    Find the pod using the PVC:

kubectl get pvc

Exec into the pod & copy data:

kubectl exec -it <pod-name> -- tar -czvf /backup.tar.gz /data
kubectl cp <pod-name>:/backup.tar.gz ./backup.tar.gz

Alternatively, use rsync:

    rsync -avz --progress <source-path> <backup-location>

Restore

    Copy data back into the pod:

    kubectl cp backup.tar.gz <pod-name>:/restore.tar.gz
    kubectl exec -it <pod-name> -- tar -xzvf /restore.tar.gz -C /data

🔹 Pros: Simple for small clusters.
🔹 Cons: Manual process, not ideal for large environments.
3. Etcd Backup & Restore (For Cluster State)

etcd stores Kubernetes cluster state, including API objects, secrets, and configurations.
Backup etcd

ETCDCTL_API=3 etcdctl snapshot save snapshot.db \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key

Restore etcd

    Stop the etcd service:

systemctl stop etcd

Restore the snapshot:

ETCDCTL_API=3 etcdctl snapshot restore snapshot.db

Restart etcd:

    systemctl start etcd

🔹 Pros: Full cluster backup.
🔹 Cons: Only applicable for self-managed etcd clusters (not for managed Kubernetes like EKS/AKS/GKE).
4. Velero (Recommended for Full Backups)

Velero is an open-source tool for backing up entire Kubernetes clusters, including PVs.
Install Velero

curl -fsSL https://github.com/vmware-tanzu/velero/releases/latest/download/velero-linux-amd64.tar.gz | tar -xzvf -C /usr/local/bin

Backup Cluster

velero backup create my-cluster-backup --include-namespaces=my-namespace

Restore Cluster

velero restore create --from-backup my-cluster-backup

🔹 Pros: Automates full cluster backup, works with cloud storage (AWS S3, GCP, Azure).
🔹 Cons: Requires setup and external storage.
5. Cloud-Specific Backup Solutions

For managed Kubernetes services, use cloud-native backup tools:

    AWS EKS: AWS Backup, Velero, EBS snapshots
    Azure AKS: Azure Backup, Velero, CSI snapshots
    Google GKE: GKE Backup, Cloud Storage snapshots

============================================================================================================================================
Chapter=> 7 (Security)

















